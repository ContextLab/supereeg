{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n# Simulate data\n\n\nIn this example, we load in a single subject example, remove electrodes that exceed\na kurtosis threshold (in place), load a model, and predict activity at all\nmodel locations.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Code source: Andrew Heusser & Lucy Owen\n# License: MIT\n\nimport superEEG as se\nimport os\nimport sys\nimport ast\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport seaborn as sns\nfrom superEEG._helpers.stats import r2z, z2r, corr_column\nimport matplotlib.pyplot as plt\n#plt.switch_backend('agg')\n\n\ndef main(n_elecs):\n    # n_samples\n    n_samples = 1000\n\n    # n_electrodes - number of electrodes for reconstructed patient - need to loop over 5:5:130\n    #n_elecs = range(5, 165, 160)\n    n_elecs = [10, 160]\n    #n_elecs = [ast.literal_eval(n_elecs)]\n\n    # m_patients - number of patients in the model - need to loop over 10:10:50\n    m_patients = [25]\n\n    # m_electrodes - number of electrodes for each patient in the model -  25:25:100\n    #m_elecs = range(5, 165, 160)\n    m_elecs = [10, 160]\n\n    iter_val = 1\n\n    # load nifti to get locations\n    gray = se.load('mini_model_nifti')\n\n    # extract locations\n    gray_locs = gray.locs\n\n\n    append_d = pd.DataFrame()\n\n    param_grid = [(p,m,n) for p in m_patients for m in m_elecs for n in n_elecs]\n\n    for p, m, n in param_grid:\n        d = []\n\n        for i in range(iter_val):\n\n\n    ############################\n\n    # 3 separate contingincies:\n\n    ############################\n\n           #  ### 1: no intersection of model locations and brain object locations ( intersection of A and B is null )\n           #\n           #  # subset locations to build model\n           #  mo_locs = gray_locs.sample(m).sort_values(['x', 'y', 'z'])\n           #\n           #  #create brain objects with m_patients and loop over the number of model locations\n           #  model_bos = [se.simulate_bo(n_samples=10000, sample_rate=1000, locs = mo_locs) for x in range(p)]\n           #\n           #  # create model from subsampled gray locations\n           #  model = se.Model(model_bos, locs=mo_locs)\n           #\n           #  # create brain object from the remaining locations - first find remaining locations\n           #  sub_locs = gray_locs[~gray_locs.index.isin(mo_locs.index)]\n           #\n           #  # create a brain object with all gray locations\n           #  bo = se.simulate_bo(n_samples=1000, sample_rate=1000, locs=gray_locs)\n           #\n           #  # get indices for unknown locations (where we wish to predict)\n           #  unknown_loc = mo_locs[~mo_locs.index.isin(sub_locs.index)]\n           #\n           #  # parse brain object to create synthetic patient data\n           #  data = bo.data.T.drop(unknown_loc.index).T\n           #\n           #  # put data and locations together in new sample brain object\n           #  bo_sample = se.Brain(data=data.as_matrix(), locs=sub_locs)\n           #\n           #  # predict activity at all unknown locations\n           #  recon = model.predict(bo_sample)\n           #\n           #  # this next step is redundant - just get from unknown_loc later\n           # # unknown_ind = [item for item in bo.data.columns if item not in data.columns]\n           #\n           #  #actual = bo.data.iloc[:, unknown_ind]\n           #  actual = bo.data.iloc[:, recon.locs.index]\n           #\n           #  corr_vals = corr_column(actual.as_matrix(), recon.data.as_matrix())\n\n    ####################################\n\n            # ### 2: all brain object locations are also model locations ( B is a subset of A)\n            #\n            # # subset gray locations to build model\n            # mo_locs = gray_locs.sample(m).sort_values(['x', 'y', 'z'])\n            #\n            # #create brain objects with m_patients and loop over the number of model locations\n            # model_bos = [se.simulate_bo(n_samples=10000, sample_rate=1000, locs = mo_locs) for x in range(p)]\n            #\n            # # create model from subsampled\n            # model = se.Model(model_bos, locs=mo_locs)\n            #\n            # # brain object locations subsetted entirely from model locations - for this m > n\n            # sub_locs = mo_locs.sample(n).sort_values(['x', 'y', 'z'])\n            #\n            # # create a brain object with all gray locations\n            # bo = se.simulate_bo(n_samples=1000, sample_rate=1000, locs=gray_locs)\n            #\n            # # get indices for unknown locations (where we wish to predict) indices for gray_locs - sub_locs\n            # unknown_loc = gray_locs[~gray_locs.index.isin(sub_locs.index)]\n            #\n            # # parse brain object to create synthetic patient data\n            # data = bo.data.T.drop(unknown_loc.index).T\n            #\n            # # put data and locations together in new sample brain object\n            # bo_sample = se.Brain(data=data.as_matrix(), locs=sub_locs)\n            #\n            # # predict activity at all unknown locations\n            # recon = model.predict(bo_sample)\n            #\n            # # sample actual data at reconstructed locations\n            # actual = bo.data.iloc[:, recon.locs.index]\n            #\n            # corr_vals = corr_column(actual.as_matrix(), recon.data.as_matrix())\n\n    ############################\n\n            ### 3: some locations in the brain object overlap with the model locations\n\n            # subset locations to build model\n            #mo_locs = gray_locs.sample(m).sort_values(['x', 'y', 'z'])\n\n            #create brain objects with m_patients and loop over the number of model locations\n            model_bos = [se.simulate_model_bos(n_samples=10000, sample_rate=1000, locs=gray_locs, sample_locs = m) for x in range(p)]\n\n            #model_bos = [se.simulate_bo(n_samples=10000, sample_rate=1000, locs = gray_locs.sample(m).sort_values(['x', 'y', 'z'])) for x in range(p)]\n\n            model_locs = pd.DataFrame()\n            for i in range(len(model_bos)):\n                #locats = model_bos[i].locs\n                model_locs = model_locs.append(model_bos[i].locs, ignore_index = True)\n\n            # create model from subsampled gray locations\n            model = se.Model(model_bos, locs=gray_locs)\n\n\n            # # brain object locations subsetted entirely from both model and gray locations - for this n > m (this isn't necessarily true, but this ensures overlap)\n            sub_locs = gray_locs.sample(n).sort_values(['x', 'y', 'z'])\n\n\n            # for the case where you want both subset and disjoint locations - get indices for unknown locations (where we wish to predict)\n            unknown_loc = gray_locs[~gray_locs.index.isin(sub_locs.index)]\n\n            bo = se.simulate_bo_random(n_samples=1000, sample_rate=1000, locs=gray_locs)\n\n            data = bo.data.T.drop(unknown_loc.index).T\n            bo_sample = se.Brain(data=data.as_matrix(), locs=sub_locs)\n\n            recon = model.predict(bo_sample)\n\n            # sample actual data at reconstructed locations\n            actual = bo.data.iloc[:, unknown_loc.index]\n\n            #correlate reconstruction with actual data\n            corr_vals = corr_column(actual.as_matrix(),recon.data.as_matrix())\n            corr_vals_sample = np.random.choice(corr_vals, 5)\n\n            d.append({'Numbder of Patients in Model': p, 'Number of Model Locations': m, 'Number of Patient Locations': n, 'Average Correlation': corr_vals_sample.mean(), 'Correlations': corr_vals, 'Model Locations': model_locs.values, 'Patient Locations': bo_sample.locs.values})\n\n        d = pd.DataFrame(d, columns = ['Numbder of Patients in Model', 'Number of Model Locations', 'Number of Patient Locations', 'Average Correlation', 'Correlations', 'Model Locations', 'Patient Locations'])\n        append_d = append_d.append(d)\n        append_d.index.rename('Iteration', inplace=True)\n\n\n    append_d\n\n\n    if os.path.isfile('ave_corrs'):\n        f = open('ave_corrs', 'a')\n        append_d.to_csv(f, mode='a', header=False)\n        f.close()\n    else:\n        f = open('ave_corrs', 'a')\n        append_d.to_csv(f, mode='a', header=True)\n        f.close()\n\n    new_df=append_d.groupby('Average Correlation').mean()\n    # new_df['Proportion of electrodes from to-be-reconstructed patient'] = new_df['Number of Model Locations'] / 170\n    # new_df['Proportion of electrodes from patients used to construct model'] = new_df['Number of Patient Locations'] / 170\n    if len(np.unique(new_df['Numbder of Patients in Model'])) > 1:\n\n        fig, axs = plt.subplots(ncols=len(np.unique(new_df['Numbder of Patients in Model'])), sharex=True, sharey=True)\n\n        axs_iter = 0\n        cbar_ax = fig.add_axes([.92, .3, .03, .4])\n        for i in np.unique(new_df['Numbder of Patients in Model']):\n\n\n            data_plot = append_d[append_d['Numbder of Patients in Model'] == i].pivot_table(index=['Number of Model Locations'], columns='Number of Patient Locations',\n                                                                  values='Average Correlation')\n            axs[axs_iter].set_title('Patients = '+ str(i))\n            sns.heatmap(data_plot, cmap=\"coolwarm\", cbar = axs_iter == 0, ax = axs[axs_iter], cbar_ax = None if axs_iter else cbar_ax)\n            axs[axs_iter].invert_yaxis()\n            axs_iter+=1\n\n    else:\n        for i in np.unique(new_df['Numbder of Patients in Model']):\n            data_plot = append_d[append_d['Numbder of Patients in Model'] == i].pivot_table(\n                index=['Number of Model Locations'], columns='Number of Patient Locations',\n                values='Average Correlation')\n            ax = sns.heatmap(data_plot, cmap=\"coolwarm\", vmin=-1, vmax=1)\n            ax.invert_yaxis()\n            ax.set(xlabel='Number of electrodes from to-be-reconstructed patient', ylabel=' Number of electrodes from patients used to construct model')\n            #axs_iter += 1\n    #\n    #\n\n    #\n    plt.savefig('average_correlation_heatmap.pdf')\n\n    ## put in locations of electrodes as well\n\n    # sns.jointplot(bo.data.iloc[:, unknown_ind].values.flatten(), predicted)\nif __name__ == \"__main__\":\n    main(sys.argv[1])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.10", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}