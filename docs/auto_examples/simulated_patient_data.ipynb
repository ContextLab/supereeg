{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\n# Simulate data\n\n\nIn this example, we load in a single subject example, remove electrodes that exceed\na kurtosis threshold (in place), load a model, and predict activity at all\nmodel locations.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Code source: Andrew Heusser & Lucy Owen\n# License: MIT\n\nimport supereeg as se\nimport scipy\nimport numpy as np\nfrom supereeg._helpers.stats import _r2z, _z2r, _corr_column, recon_no_expand\nfrom supereeg._helpers.bookkeeping import slice_list\nfrom numpy import inf\nfrom scipy.stats import zscore\nfrom scipy.spatial.distance import squareform, pdist\nimport os\nimport pandas as pd\nimport pickle\nimport seaborn as sb\nimport sklearn\n\n# n_samples\nn_samples = 1000\n\n# n_electrodes - number of electrodes for reconstructed patient - need to loop over 5:5:130\nn_elecs =[165, 85, 5]\n#n_elecs = [165]\n\n# m_patients - number of patients in the model - need to loop over 10:10:50\nm_patients = [50, 40]\n\n# m_electrodes - number of electrodes for each patient in the model -  25:25:100\nm_elecs = 170\n\n# load nifti to get locations\ngray = se.load(os.path.dirname(os.path.abspath(__file__)) + '/../supereeg/data/gray_mask_20mm_brain.nii')\n\n# extract locations\nlocs = gray.locs\n\nimport supereeg as se\n# small model\nmodel = se.load('example_model')\ndata = se.load('example_data')\n\n\n#### starting to predict parallelization\nreconstruct = model.predict(data)\n\nrecon_parallel = model.predict(data, parallel=True)\n\n# create directory for synthetic patient data\nsynth_dir = os.path.dirname(os.path.abspath(__file__)) + '/../supereeg/data/synthetic_data'\nif not os.path.isdir(synth_dir):\n    os.mkdir(synth_dir)\n\n# create 50 synthetic patients data with activity at every location\nif not os.listdir(synth_dir):\n\n    ### for toeplitz matrix\n    # R = scipy.linalg.toeplitz(np.linspace(0,1,len(locs))[::-1])\n\n    ### for distance matrix\n    D = squareform(pdist(locs))\n    R = np.max(D) - D\n    R = R - np.min(R)\n    R = R / np.max(R)\n\n    count = 0\n    for ps in range(50):\n        rand_dist = np.random.multivariate_normal(np.zeros(len(locs)), np.eye(len(locs)), size=n_samples)\n        bo = se.Brain(data=np.dot(rand_dist, scipy.linalg.cholesky(R)), locs=pd.DataFrame(locs, columns=['x', 'y', 'z']))\n        bo.save(os.path.join(synth_dir, 'synthetic_'+ str(count).rjust(2, '0')))\n        count += 1\nelse:\n    print os.listdir(synth_dir)\n\n\n# initiate model\nmodel_data = []\n\n# initiate dataframe\nd = []\n\nfor p in m_patients:\n\n    # random sample m_patients from 50 simulated patients - this will also need to be in a loop\n    patients = np.random.choice(range(50), p, replace=False)\n\n    # loop over n_elecs\n    for n in n_elecs:\n\n        # hold out one patient at a time\n        for i in patients:\n\n            # random sample n locations from 170 locations\n            p_n_elecs = np.sort(np.random.choice(range(len(locs)), n, replace=False))\n\n            ### to debug expand_corrmat:\n            # p_n_elecs = range(10,15)\n\n            with open(os.path.join(synth_dir, 'synthetic_'+ str(i).rjust(2, '0') + '.bo'), 'rb') as handle:\n                bo_actual = pickle.load(handle)\n                bo_sub = se.Brain(data=bo_actual.data.loc[:, p_n_elecs],locs= bo_actual.locs.loc[p_n_elecs])\n\n\n            unknown_locs = locs.drop(p_n_elecs)\n            unknown_inds = unknown_locs.index.values\n\n            ##### create model from every other patient\n            # model_patients = [p for p in patients if p != i]\n            # for mp in model_patients:\n            #\n            #     # random sample m_elecs locations from 170 locations (this will also need to be looped over for coverage simulation)\n            #     p_m_elecs = np.sort(np.random.choice(range(len(locs)), m_elecs, replace=False))\n            #\n            #     with open(os.path.join(synth_dir, 'synthetic_' + str(mp).rjust(2, '0') + '.bo'), 'rb') as handle:\n            #         bo = pickle.load(handle)\n            #         model_data.append(se.Brain(data=bo.data.loc[:, p_m_elecs], locs=bo.locs.loc[p_m_elecs]))\n            #\n            # model = se.Model(data=model_data, locs=locs)\n\n            #### to use simulated model\n            with open('model_170.mo', 'rb') as a:\n                model = pickle.load(a)\n\n\n            # #### comparing second corrmat_expand\n            # #### expand all\n            # reconstructed_predict = model.predict(bo_sub)\n            # #### only expand into unknownxknown and knownxknown\n            # reconstructed_fit = model.predict(bo_sub, prediction=True)\n            # #### check if they give the same values\n            # corr_reconstructions = np.mean(_corr_column(reconstructed_predict.data.as_matrix(), reconstructed_fit.data.as_matrix()))\n            # #### comparing second corrmat_expand\n\n            reconstructed_predict = model.predict(bo_sub)\n\n            ##### to use predict function (averaging the subject's expanded matrix with the model) but bypass the second expanded\n            reconstructed = model.predict(bo_sub, simulation=True)\n            predicted = reconstructed.data.as_matrix()\n\n            corr_reconstructions = np.mean(_corr_column(reconstructed_predict.data.as_matrix(), predicted))\n            ##### to bypass predict function entirely (and only parse model):\n            # predicted = recon_no_expand(bo_sub, model)\n\n            actual = zscore(bo_actual.data.loc[:, unknown_inds].as_matrix())\n\n            corr_vals = _corr_column(actual, predicted)\n\n\n            d.append({'Patients': p, 'Model Locations': m_elecs, 'Patient Locations': n, 'Correlation': np.mean(corr_vals)})\n\nd = pd.DataFrame(d)\n\n\n\n\n###### to debug the expand_corrmat mode ='predict'\n\n\n\n\n# ### reconstruction with model (no expanding)\n# for n in n_elecs:\n#\n#     ## to create a model from all 50 simulated patients\n#     for i in patients:\n#\n#         p_n_elecs = np.sort(np.random.choice(range(len(locs)), n, replace=False))\n#\n#         with open(os.path.join(synth_dir, 'synthetic_'+ str(i).rjust(2, '0') + '.bo'), 'rb') as handle:\n#             bo_actual = pickle.load(handle)\n#             bo_sub = se.Brain(data=bo_actual.data.loc[:, p_n_elecs],locs= bo_actual.locs.loc[p_n_elecs])\n#\n#         if not os.path.isfile('model_170.mo'):\n#             # create model from every patient\n#             model_patients = [p for p in patients]\n#             for m in model_patients:\n#\n#                 with open(os.path.join(synth_dir, 'synthetic_' + str(m).rjust(2, '0') + '.bo'), 'rb') as handle:\n#                     bo = pickle.load(handle)\n#                     #model_data.append(se.Brain(data=bo.data.loc[:, unknown_inds], locs=bo.locs.loc[unknown_inds]))\n#                     model_data.append(se.Brain(data=bo.data, locs=bo.locs))\n#\n#             model = se.Model(data=model_data, locs=locs)\n#\n#             with open('model_170.mo', 'wb') as h:\n#                 pickle.dump(model, h)\n#\n#         with open('model_170.mo', 'rb') as a:\n#             mo = pickle.load(a)\n#\n#         def recon_no_expand(bo_sub, mo):\n#             \"\"\"\n#             \"\"\"\n#             model = _z2r(np.divide(mo.numerator, mo.denominator))\n#             model[np.eye(model.shape[0]) == 1] = 1\n#             known_locs = bo_sub.locs\n#             known_inds = bo_sub.locs.index.values\n#             unknown_locs = mo.locs.drop(known_inds)\n#             unknown_inds = unknown_locs.index.values\n#             Kba = model[unknown_inds, :][:, known_inds]\n#             Kaa = model[:,known_inds][known_inds,:]\n#             Y = zscore(bo_sub.get_data())\n#             return np.squeeze(np.dot(np.dot(Kba, np.linalg.pinv(Kaa)), Y.T).T)\n#\n#\n#         def corr(X, Y):\n#             return np.array([scipy.stats.pearsonr(x, y)[0] for x, y in zip(X.T, Y.T)])\n#\n#\n#         predicted = np.atleast_2d(recon_no_expand(bo_sub, mo))\n#\n#         unknown_locs = locs.drop(p_n_elecs)\n#         unknown_inds = unknown_locs.index.values\n#\n#         actual = zscore(bo_actual.data.loc[:, unknown_inds].as_matrix())\n#\n#         corr_vals = corr(actual, predicted)\n#\n#\n#         d.append({'Patients': m_patients, 'Model Locations': m_elecs, 'Patient Locations': n, 'Correlation': np.mean(corr_vals)})\n#\n# d = pd.DataFrame(d)\n#\n# recon_corr = d.groupby(['Patient Locations']).mean()\n#\n\n\n\n\n\n\n### create model with synthetic patient data, random sample 20 electrodes\n#\n# R = scipy.linalg.toeplitz(np.linspace(0,1,len(locs))[::-1])\n# data = []\n# for i in range(50):\n#\n#     p = np.random.choice(range(len(locs)), 20, replace=False)\n#\n#     rand_dist = np.random.multivariate_normal(np.zeros(len(locs)), np.eye(len(locs)), size=n_samples)\n#     data.append(se.Brain(data=np.dot(rand_dist, scipy.linalg.cholesky(R))[:,p], locs=pd.DataFrame(locs[p,:], columns=['x', 'y', 'z'])))\n#\n#     #bo.to_pickle(os.path.dirname(os.path.abspath(__file__)) + '/../supereeg/data/synthetic_' + str(i))\n# model = se.Model(data=data, locs=locs)\n\n# ## create brain object to be reconstructed\n# ## find indices\n# locs_inds = range(0,len(locs))\n# sub_inds = np.sort(np.random.choice(range(len(locs)), 20, replace=False))\n# unknown_inds = list(set(locs_inds)-set(sub_inds))\n#\n# rand_dist = np.random.multivariate_normal(np.zeros(len(locs)), np.eye(len(locs)), size=n_samples)\n# full_data = np.dot(rand_dist, scipy.linalg.cholesky(R))\n# bo_sub = se.Brain(data=full_data[:, sub_inds], locs=pd.DataFrame(locs[sub_inds, :], columns=['x', 'y', 'z']))\n# bo_actual = se.Brain(data=full_data, locs=pd.DataFrame(locs, columns=['x', 'y', 'z']))\n#\n# ## need to figure out if we want to keep the activty used to predict - right now its added at the end, but that might not be the best\n# reconstructed = model.predict(bo_sub)\n#\n# import seaborn as sb\n# sb.jointplot(reconstructed.data, bo_actual.data)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.11", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}